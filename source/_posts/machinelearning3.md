---
title: 什么是机器学习以及相关的概念
date: 2018-11-09 10:16:31
categories:
- machine learning
tags:
- 周志华
- machine learning
- 西瓜书
---
今日做了一个决定，要将周志华老师写的《西瓜书》重新看一遍。额，别多想，之前也没看完，只是看了一半。
但是，因为现在对机器学习的理解加深了一层，而且每一次看都有不同的感悟，趁着自己还想多学点东西，就再学习吧。
文章的内容主要是参考《西瓜书》，当然并不限于。我会综合很多我看过的东西来写博客，为自己也为后来者。
<!-- more -->
在学习机器学习之前，我推荐你看一本书，这也是我入门的第一本书。
[大数据时代](https://book.douban.com/subject/20429677/)
我也想简单的介绍一下机器学习到底是什么？
## 机器学习到底是什么？
东野圭吾写过一本小说叫《解忧杂货铺》，里面有一个情节就是当有人提出想要见一下那三个人的时候，那三个人说：还是不要见的好，因为你见了我们之后，你就会觉得原来一直给我建议的竟然是这种人。
是的，隔行如隔山，初次听到机器学习你觉得，WOW，牛逼，厉害。但是，了解了之后，你就会发现原来机器学习是这种东西。
我还是用《大数据时代》中的例子作为开场白。
假设有一个偷窥狂，他收集了一个人五年中9:00所做的事情（偷窥狂都这么努力，你还有什么理由放弃。。。）。他发现了一个规律，就是在五年期间，那个人 97% 在上班，3% 在家中。于是，偷窥狂想明天这个人 9:00 会干什么？
毫无疑问，偷窥狂会选择他去上班。
上述的例子中，偷窥狂预测那个人未来要做的事。而无论是大数据还是机器学习的核心就是预测。
所谓的预测也很容易，只是对经验的一些延展。大数据重点在于海量数据的支持下预测未来发生的事情，是对规律的探索。机器学习是在海量的数据支持下，研究出某种模型，从而对没有见过的样例产生概率性预测。
当然，他们两者并不是只有预测功能。
另外，我们在生活中无时无刻都在预测，我们看惯了物体由于重力的原因自由落体，如果我们手中的杯子脱离了掌控力，那么毫无疑问，我们会下意识地知道这个杯子将亲吻大地。
所以，机器学习和大数据并不神秘，他们无时无刻的发生在我们身边。
由已知的过去去推可以描述的未来。
## 西瓜书的定义
讲道理，我这个人非常讨厌定义，我更加侧重于自身的理解，但是，大环境就是这样，没有办法。
OK，请看。算是为后人造福。。。如果你觉得有用，请给我打钱，我是秦始皇。。。
### 特征
特征就是描述样本的数据，没有那么复杂，比如一个人，有两个眼睛，两个鼻子，一个嘴，这些都是特征。
鸟有翅膀。所以，我们可以根据这些特征来区分事物。
在计算机中特征当然没有那么具象的表述（当然，也可以有），但是我们通常会这样表述，翅膀：{0,1} 。
一个样本的特征有很多，我们把里面具体的东西叫做属性，比如，翅膀，眼睛等等。
通常，在机器学习中，我们的特征会用一个向量表示。当然，我还是要提一点，向量是的主方向是竖着的。
当然，具体的样本矩阵的行是样本，竖是特征。所以，在做算法编程的时候，要尤为注意这些细节。
### 预测的类型
如果我们要预测的是离散值，那么这类学习任务称为分类。比如预测你是男人或者女人。因为这两个标签并没有什么联系，所以是离散的。
如果我们预测的是连续值，那么这类学习是回归问题，比如我们预测一个函数的最优解，因为在解空间中是连续的。
## 监督和非监督
听听这两个专业词汇是不是很懵逼，哈哈，不要着急。
所以的监督就是有标签的数据。那么标签是什么呢？
比如，我们有很多图片，我们给狗的图片标注这是狗，给猫的图片标注这是猫。拥有具体标明的数据就是图片。
我很懂你的意思，就是怎么打标签？
其实，所谓的标签怎么打完全是数据制造者的意思，比如，我们的图片分类数据是 {******,1} 其中 * 是属性，而那个 1 就是代表某一种具体的标注，比如狗。
有时候我们也会遇到数据和标签在不同的文件中，当然，这都无所谓。
那么非监督当然是没有标签的数据。
通常分类和回归是典型的监督问题，而聚类是典型的非监督问题。
## 泛化
所谓的泛化就是你创建的模型的预测率。
是形容你这个模型好坏的指标。
## 假设空间
即所有可能的假设例子，也就是你的属性取所有可能值的空间，当然，我们要注意一点是，属性可以取 * ，也就是所有属性。
比如，狗的毛色，白，黄，黑。但是在假设空间中，我们的毛色可以表示为：白，黄，黑，*。
## 版本空间
这个定义，我估计是老周自己想的。
具体如何求版本空间，老周这这样描述的。

	搜索过程中可以不断删除与正例不一致的假设，与反例一致的假设。
	最终将会获得与训练集一致的假设，这就是我们学到的结果。
	
懵不懵，我将举一个具体的例子。
看下面的样本空间

		皮		色泽	好橘子
	1	黄		亮		是
	2	绿		暗		不是
	
所以，我们全部的样本空间中样本总数是 3 * 3 + 1 = 10，加的那个 1 是空集。
所以所有的样本例子如下：

	黄	亮
	绿	亮
	*	亮
	黄	暗
	绿	暗
	*	暗
	*	*
	
OK，我们现在开始按照规则去制造版本空间。
首先我们根据样本 1，也就是

	黄	亮	是
	
删除和这个样本不一样的例子，也就是不包含这个例子的。
我们得到剩下的样本空间：

	黄	亮
	*	亮
	*	*
	
我们再根据样本 2 删除包含的例子，剩下的样本空间如下：

	黄	亮
	*	亮
	
这就是最后的版本空间。
所以，理解版本空间的意义就非常重要，也就是说，我们所得到的版本空间是根据训练集来的。
反正，也不想写了，意会吧，因为周围的环境太嘈杂了，放弃思考。。。
## 奥卡姆剃刀(Occam's razor)
奥卡姆剃刀是研究的基本原则。

	若有多个假设和观察一致，则选择最简单的那个。
	
抽不抽象？看例子。
假设（仅仅只是假设）我们知道一个版本空间如下

	黄	亮	好
	黄	*	坏

我们现在有这样一个特征向量 {黄，亮}，如果按照第一个版本样本，那么就是好的，如果按照版本空间中的第二个那就是坏的。
所以，机器学习一般都是带偏置的。如果你的模型偏置是趋向于一般，那么这就是坏的，如果你的模型趋向于特殊，那这就是好的。
但是，在现实生活的问题求解中，我们应该怎样选择。通常我们会使用奥卡姆剃刀原则，即相同模型下，选择更简单的那个。
OK，现在选择原则有了，但是却有一个更大的问题，那就是什么是更简单？
事实上，目前还没有一个理论说明更简单到底指什么，所以，问题的求解还是依照于使用者的目的以及趋向。




