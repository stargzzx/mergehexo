---
title: 模型评估和选择
date: 2018-11-10 11:18:36
categories:
- [人工智能,深度学习,基础]
tags:
- 周志华
- machine learning
- 西瓜书
---
OK，这一章我们的内容也是和题目一样，虽然大部分在说定义，但是却十分重要。

<!-- more -->

## 经验误差

学习器在训练集上的误差称为“训练误差”或“经验误差”

在新样本上的误差称为“泛化误差”。

过拟合是无法避免的，我们能做的的只是“缓解”，或者说减小其风险。

## 评估方法

我们首先要知道的是，我们所得到的模型最后是给未知样本使用的。但是，在现实中我们只有训练数据，而没有未来数据。

所以，我们会将测试数据进行分割，变成训练数据和测试数据。

关于如何挑选测试数据，有以下的原则：

	测试集和训练集要互斥
	测试集，训练集要尽可能保持数据分布的一致性。
		比如：有一个数据集 T，它有 500 个正例，300个反例，所以应该给训练集分 400个正例，250 个反例，给测试集 100 个正例 50 个反例
		即尽可能保证训练集和测试集的类别比例差不多
		
但是如何挑选测试集和训练集呢？以下给出三种方法

## 挑选训练集和测试集方法

### 留出法(hold out)

留出法是直接将数据集 D 划分成两个互斥的集合，训练集 S，测试集 T。


一般训练集 S 占据 2/5 ~ 4/5

但是，有一个关键问题在于，我们想训练的数据模型应该尽可能囊括所有的数据集，即，数据量越多，你训练的模型就会越好。

但是，留出法却将一部分数据不参与训练，导致训练出的模型具有局部性质。

### 交叉验证法

这个是将数据集 D 划分成 K 个大小相似的子集。

然后每一次选用 K-1 的子集作为训练集，剩下的那个子集作为测试集，循环 K 次。

最终评估的结果是 K 折交叉验证结果的均值。

当然，特殊一点的是留一法(leave one out,LOO)，即测试集只有 1 个样本。

因为留一法几乎囊括了所有的训练数据，所以，它训练出的模型具有代表性，但是遇到数据量大的，就很吃力。

### 自助法

这是一个很有意思的方法。

具体如下：

给定 m 个样本的数据集 D，我们对其采样产生数据集 D'.

每次随机从 D 中选取一个样本放在 D' 中，然后将样本放回 D 中，以保证下一次还有可能选到，这个过程重复 m 次，所得的 D' 就是训练数据集。

经过数学计算，初始数据集 D 约有 36.8% 的样本不会出现在 D' 中。

然后用 D - D' 作为测试集。

一般来说，自助法在数据集较小，难以有效划分数据集和测试集时比较有效。如果初始数据集量足够，则推荐使用留出法或者交叉验证法。

## 调参

机器学习的大部分工作都在无聊的调参上，实在是让人想吐。

但是我们如何要验证参数的好坏呢？测试集是为了验证模型的好坏，这个是不能动的。

所以，我们只能从训练集上再划分一块，即定义为验证集。

我们会基于验证集上的性能来进行模型的选择和调参。

## 查准率和查全率

我们可以用准确率，错误率来评价一个模型的好坏。

但是，这样分类并不能满足生活中所有的要求。

比如，我想知道预测的例子中正确的占比是多少？预测的例子中挑选的正确例子占所有正确例子的比例是多少？

再回答这个问题前，我想先介绍几个概念。

	真正例	true positive 	TP
	假正例	false positive	FP
	真反例	true negative	TN
	价反例	false negative	FN
					预测结果
	真实情况	T				F
		T		TP				FN
		F		FP				TN
	其中 TP + TN + FN + FP = 样例总数

### 查准率(准确率)

查准率(precision)，比如对于搜索引擎来说是检索出来的有多少时用户感兴趣的。

	P = TP / (TP + FP)
	用户真正感兴趣的数量 / 预测出用户感兴趣的数量
	
### 查全率(召回率)

查全率(recall)，比如对于搜索引擎来说，用户感兴趣的有多少被查出。

	R = TP / (TP + FN)
	用户真正感兴趣的量 / 真实情况下的正例
	
通常查准率和查全率是一对冤家，即查准率高那么查全率就低，这个用逻辑也能思考出来，就不过多陈述了。。。

### P-R 曲线

我们有个 P 和 R 的定义，那么我们就可以做出它们的关系曲线。

我们根据模型来预测样例，并对他们的结果进行排序，最前面的是最可能的，最后面的是最不可能的。

然后按照这个顺序，把每个样本作为正例进行预测，算出 P R 然后做曲线。

如下图所示：

![](/images/machinelearning/4_0.JPG)

如果某一个模型的 S(P-R) 大，那么就说明这个模型性能好。S 是指面积。

但是我们知道用积分求面积的话，通常不会很容易，所以我们引入一个变量 F1。

### F1

	F1 = (2 * P * R) / (P + R)
		= (2 * TP) / (样例总数 + TP - TN)
		
对于更一般的式子如下：

	F(β) = [(1 + β^2) * P * R] / [(β^2 * P) + R]
	β > 1 查全率有更大的影响
	β < 1 查准率有更大的影响
	β = 1 则为标准的 F1
	
对于衡量一个模型的好坏，我们通常取多次测试，然后取平均值。




