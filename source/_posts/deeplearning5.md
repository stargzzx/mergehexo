---
title: 神经网络初篇
date: 2018-11-08 17:29:52
categories:
- deep learning
tags:
- machine learning
- neural network
- nn
- deep learning
---
正如人的大脑，大脑内含有数不清的神经突触（元），每一刻都从世界中获得大量的信息源，当一个神经元获得大量的信息后，会经过自身处理，将处理完的结果继续传播，而不是传原来的信息，所以经过一个又一个神经元处理后，可以处理大量数据。
<!-- more -->
神经网络大致可以分为三层：一层输入，一层输出，N层隐藏。
神经网络的核心就是梯度下降（Gradient Descent）。
所谓的 Gradient 就是求导，求微分。
其实梯度下降是优化算法的分支，我们甚至可以称神经网络也是优化算法。。。
OK，我们来讲一下梯度下降到底是什么。。。其实，并不想讲，但是，为了巩固，额，我就讲一下吧。
## 梯度下降
刚才翻了一下我的博客，我才发现原来之前有写过。。。
[Logistic回归](https://benpaodewoniu.github.io/2018/06/19/machinelearning-algorithm3/)
## 神经网络的黑盒过程
假设我们有一层输入，两层隐藏，一层输出。
假设我们传进去一个图片，在输入层这张图片我们人类可以理解，但是电脑并不能理解。
于是经过第一层的隐藏层后，我们提取特征，计算机将一张图片变成它所能理解的方式，然后，经过第二层的隐藏层，计算机将上一层的特征再进行提取。然后根据所提取的特征，计算机就可以对图片做后置处理。
比如，分类等等。























